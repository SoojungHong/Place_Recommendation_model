{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11409\n",
      "Collaborative filtering AUC 0.904, Popularity baseline AUC 0.945\n",
      "current set size :  5\n",
      "set([\"'7-Eleven'\", 'None', \"'Whole Foods'\", '\"McDonald\\'s\"', \"'Starbucks'\"])\n",
      " current similarity score :  0.8\n",
      "35449\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import implicit\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# data and header\n",
    "#--------------------\n",
    "CHICAGO_DATA = 'C:/Users/shong/Documents/data/chicago1year_train_clean_newclients_dedupe_chain_restaurants-1.tsv'\n",
    "BERLIN_DATA = 'C:/Users/shong/Documents/data/berlin1year_train_clean_newclients_dedupe_chain_center_restaurants.tsv'\n",
    "NY_DATA = 'C:/Users/shong/Documents/data/newyork1year2x_train_clean_newclients_dedupe_chain_center_restaurants.tsv'\n",
    "CHICAGO_HEADER = ['cookie', 'ppid', 'result_provider', 'click_time', 'query_string', 'query_language', 'categories', 'place_name', 'lat', 'lon', 'click_order', 'chain_info']\n",
    "BERLIN_HEADER = ['cookie', 'ppid', 'result_provider', 'click_time', 'query_string', 'query_language', 'categories', 'place_name', 'lat', 'lon', 'click_order', 'chain_info', 'unknown_col1', 'unknown_col2']\n",
    "NY_HEADER =['cookie', 'ppid', 'result_provider', 'click_time', 'query_string', 'query_language', 'categories', 'place_name', 'lat', 'lon', 'click_order', 'chain_info', 'unknown_col1', 'unknown_col2']\n",
    "\n",
    "\n",
    "def readDataset(): \n",
    "    file_path = 'C:/Users/shong/Documents/data/chicago1year_train_clean_newclients_dedupe_chain_restaurants-1.tsv'\n",
    "    headers = ['cookie', 'ppid', 'result_provider', 'click_time', 'query_string', 'query_language', 'categories', 'place_name', 'lat', 'lon', 'click_order', 'chain_info']\n",
    "    data = pd.read_csv(file_path, sep='\t', names=headers, error_bad_lines=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def readData(dataset, header):\n",
    "    file_path = dataset\n",
    "    headers = header\n",
    "    data = pd.read_csv(file_path, sep='\t', names=headers, error_bad_lines=False)\n",
    "    return data \n",
    "\n",
    "\n",
    "\n",
    "def removeSpecialChar(chars): \n",
    "    for char in chars:\n",
    "        if char in \"\\,?.!/;:()\":\n",
    "            chars = chars.replace(char,'')\n",
    "    return chars\n",
    "\n",
    "\n",
    "\n",
    "def removeSpecialCharactor(chars): \n",
    "    trim_chars = removeSpecialChar(chars)\n",
    "    return trim_chars\n",
    "\n",
    "\n",
    "\n",
    "def removeSpecialCharFromChainName(chars): \n",
    "    trim_chars = removeSpecialChar(chars)\n",
    "    return trim_chars\n",
    "\n",
    "\n",
    "def parseChainId(chain_text, ppid): \n",
    "    chainInfos = chain_text.split(\",\")\n",
    "    chainName = chainInfos[0] \n",
    "    if(chainName == 'None'):\n",
    "        chainId = ppid\n",
    "    else:\n",
    "        chainId = removeSpecialChar(chainInfos[1])\n",
    "    return chainId\n",
    "\n",
    "\n",
    "\n",
    "def isQueryMatchChain(query, s):\n",
    "    isMatch = False\n",
    "    for setEle in s:\n",
    "        ratio = SequenceMatcher(a=query,b=setEle).ratio()\n",
    "        if(ratio > 0.9):\n",
    "            isMatch = True\n",
    "    return isMatch        \n",
    "\n",
    "\n",
    "\n",
    "def isQueryMatchChainWithScore(query, s, score):\n",
    "    isMatch = False\n",
    "    for setEle in s:\n",
    "        ratio = SequenceMatcher(a=query,b=setEle).ratio()\n",
    "        if(ratio > score):\n",
    "            isMatch = True\n",
    "    return isMatch        \n",
    "\n",
    "\n",
    "\n",
    "def parseChainIdwithQuerySimilarity(query_str, chain_text, ppid, s, score): \n",
    "    chainInfos = chain_text.split(\",\")\n",
    "    chainName = chainInfos[0] \n",
    "    if(chainName == 'None'):\n",
    "        chainId = ppid\n",
    "    else:    \n",
    "        if(isQueryMatchChainWithScore(query_str, s, score) == True): \n",
    "            chainId = removeSpecialCharactor(chainInfos[1])\n",
    "        else:\n",
    "            chainId = ppid     \n",
    "    return chainId\n",
    "\n",
    "\n",
    "\n",
    "def labelOnItemUsingChainIdwithQuerySimilarity(query_str, chain_text, ppid, s, score): \n",
    "    chainInfos = chain_text.split(\",\")\n",
    "    chainName = chainInfos[0] \n",
    "    if(chainName == 'None'):\n",
    "        placeLabel = 'NO_CHAIN'\n",
    "    else:    \n",
    "        if(isQueryMatchChainWithScore(query_str, s, score) == True):\n",
    "            placeLabel = removeSpecialCharFromChainName(chainName)\n",
    "        else:\n",
    "            placeLabel = 'NO_CHAIN'     \n",
    "    return placeLabel\n",
    "\n",
    "\n",
    "\n",
    "def parseChainName(chain_text): \n",
    "    chainName = ''\n",
    "    chainInfos = chain_text.split(\",\")\n",
    "    name = chainInfos[0] \n",
    "    if(name == 'None'):\n",
    "        chainName = 'None'\n",
    "    else:\n",
    "        chainName = removeSpecialChar(chainInfos[0])\n",
    "    return chainName\n",
    "\n",
    "\n",
    "\n",
    "def combineLocationInfo(chain_id, ppid_int):\n",
    "    return str(chain_id) + \"_\" + str(ppid_int)    \n",
    "\n",
    "\n",
    "\n",
    "def get_occurences(records):\n",
    "    occurs = {}\n",
    "    for record in records:\n",
    "        if record not in occurs:\n",
    "            occurs[record] = 1\n",
    "        else:\n",
    "            occurs[record] += 1\n",
    "    \n",
    "    return occurs\n",
    "\n",
    "\n",
    "\n",
    "def constructUserPlaceMatrix(cell, row, col): \n",
    "    item_user_data = csr_matrix((cell, (row, col)))\n",
    "    \n",
    "    nonzero_inds = item_user_data.nonzero()\n",
    "    items_nz = nonzero_inds[0]\n",
    "    users_nz = nonzero_inds[1]\n",
    "    \n",
    "    # filter data according to thresolds\n",
    "    threshold_items = 5\n",
    "    threshold_users = 5\n",
    "    \n",
    "    items = get_occurences(items_nz)\n",
    "    users = get_occurences(users_nz)\n",
    "    \n",
    "    for item, entry in enumerate(item_user_data):\n",
    "        for user in entry.indices:\n",
    "            if items[item] < threshold_items or users[user] < threshold_users:\n",
    "                item_user_data[item, user] = 0\n",
    "    \n",
    "    item_user_data.eliminate_zeros()\n",
    "    return item_user_data\n",
    "\n",
    "\n",
    "\n",
    "def getAllItemsWithCounts(cell, row, col): \n",
    "    item_user_data = csr_matrix((cell, (row, col)))\n",
    "    nonzero_inds = item_user_data.nonzero()\n",
    "    items_nz = nonzero_inds[0]\n",
    "    \n",
    "    # filter data according to thresolds\n",
    "    items = get_occurences(items_nz)\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "def matrixSparsity(item_user_data): \n",
    "    matrix_size = item_user_data.shape[0]*item_user_data.shape[1]\n",
    "    num_interactions = len(item_user_data.nonzero()[0])\n",
    "    sparsity = 100*(1 - (num_interactions/float(matrix_size)))\n",
    "    \n",
    "    print \"Matrix sparsity is %f\" % sparsity    \n",
    "\n",
    "\n",
    "\n",
    "def make_train(ratings, pct_test=0.2):\n",
    "    #split between training and test set and user rows that have been altered in the test set\n",
    "    #from https://jessesw.com/Rec-System/\n",
    "    \n",
    "    test_set = ratings.copy()\n",
    "    test_set[test_set != 0] = 1 \n",
    "    training_set = ratings.copy() \n",
    "    nonzero_inds = training_set.nonzero()\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1]))\n",
    "    #random.seed(0)\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) \n",
    "    samples = random.sample(nonzero_pairs, num_samples)\n",
    "    user_inds = [index[1] for index in samples] \n",
    "    item_inds = [index[0] for index in samples] \n",
    "    \n",
    "    training_set[item_inds, user_inds] = 0\n",
    "    training_set.eliminate_zeros()\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered\n",
    "\n",
    "\n",
    "\n",
    "def get_random_element_filtered(item_user_data): \n",
    "    nonzero_inds = item_user_data.nonzero()\n",
    "    items_nz = nonzero_inds[0]\n",
    "    \n",
    "    # filter data according to thresolds\n",
    "    threshold_items = 5\n",
    "    items = get_occurences(items_nz)\n",
    "    \n",
    "    valid_ppids = [key for (key, value) in items.iteritems() if value >= threshold_items]\n",
    "    return random.choice(valid_ppids)\n",
    "\n",
    "\n",
    "\n",
    "def get_similar_items(model, item_id, number_items=10):\n",
    "    similar = model.similar_items(item_id, N=number_items)\n",
    "    items = [pair[0] for pair in similar]\n",
    "    scores = [pair[1] for pair in similar]\n",
    "    return items, scores\n",
    "\n",
    "\n",
    "\n",
    "def print_similarity(items, scores, data):\n",
    "    for index in range(0, len(items)):\n",
    "        print scores[index], data[data['ppid_int'] == items[index]][['place_name', 'ppid', 'lat', 'lon']].head(1)\n",
    "\n",
    "\n",
    "\n",
    "def auc_score(predictions, test):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "\n",
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    #from https://jessesw.com/Rec-System/\n",
    "    \n",
    "    store_auc = [] \n",
    "    popularity_auc = []\n",
    "    pop_items = np.array(test_set.sum(axis = 1)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    print len(pop_items)\n",
    "    item_vecs = predictions[0]\n",
    "    for user in altered_users: \n",
    "        training_row = training_set[:,user].toarray().reshape(-1) \n",
    "        zero_inds = np.where(training_row == 0) \n",
    "        \n",
    "        user_vec = predictions[1][:,user]\n",
    "        pred = item_vecs.dot(user_vec).toarray()[zero_inds].reshape(-1)\n",
    "        \n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[:,user].toarray()[zero_inds].reshape(-1)\n",
    "        \n",
    "        pop = pop_items[zero_inds] \n",
    "        try:\n",
    "            store_auc.append(auc_score(pred, actual))\n",
    "        except ValueError:\n",
    "            print(pred, actual)\n",
    "        popularity_auc.append(auc_score(pop, actual)) \n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "\n",
    "\n",
    "\n",
    "def fitAndPredict(item_user_data):\n",
    "    alpha = 10\n",
    "    training_data, test_data, altered_users = make_train(item_user_data)\n",
    "    \n",
    "    model = implicit.als.AlternatingLeastSquares(factors=15, regularization=0.1, iterations=10)\n",
    "    training_data = training_data.astype(np.float)        \n",
    "    model.fit(training_data*alpha)\n",
    "    \n",
    "    item_recommend = get_random_element_filtered(item_user_data)\n",
    "    similar_items, similar_scores = get_similar_items(model, item_recommend, 10)\n",
    "    \n",
    "    item_vecs = model.item_factors\n",
    "    user_vecs = model.user_factors\n",
    "    \n",
    "    cf_auc, baseline_auc = calc_mean_auc(training_data, altered_users,[csr_matrix(item_vecs), csr_matrix(user_vecs.T)], test_data)\n",
    "    \n",
    "    print \"Collaborative filtering AUC %.3f, Popularity baseline AUC %.3f\" % (cf_auc, baseline_auc)\n",
    "    return cf_auc, baseline_auc\n",
    "\n",
    "\n",
    "\n",
    "def getSimilarItems(item_user_data, itemNum):\n",
    "    alpha = 10\n",
    "    training_data, test_data, altered_users = make_train(item_user_data)\n",
    "    \n",
    "    model = implicit.als.AlternatingLeastSquares(factors=15, regularization=0.1, iterations=10)\n",
    "    training_data = training_data.astype(np.float)        \n",
    "    model.fit(training_data*alpha)\n",
    "    \n",
    "    similar_items, similar_scores = get_similar_items(model, itemNum, 10)\n",
    "    \n",
    "    item_vecs = model.item_factors\n",
    "    user_vecs = model.user_factors\n",
    "    \n",
    "    cf_auc, baseline_auc = calc_mean_auc(training_data, altered_users,[csr_matrix(item_vecs), csr_matrix(user_vecs.T)], test_data)\n",
    "    \n",
    "    print \"Collaborative filtering AUC %.3f, Popularity baseline AUC %.3f\" % (cf_auc, baseline_auc)\n",
    "    return similar_items, similar_scores\n",
    "\n",
    "\n",
    "\n",
    "def getSimilarChainItems(item_user_data, itemNum):\n",
    "    alpha = 10\n",
    "    training_data, test_data, altered_users = make_train(item_user_data)\n",
    "    \n",
    "    model = implicit.als.AlternatingLeastSquares(factors=15, regularization=0.1, iterations=10)\n",
    "    training_data = training_data.astype(np.float)        \n",
    "    model.fit(training_data*alpha)\n",
    "    \n",
    "    similar_items, similar_scores = get_similar_items(model, itemNum, 10)\n",
    "    simItemSet = set()\n",
    "    for simItem in similar_items:\n",
    "        items = data_w_chain_feature[data_w_chain_feature['chain_int'] == simItem].place_name\n",
    "        #print items.values[0]\n",
    "        simItemSet.add(items.values[0])   \n",
    "    \n",
    "    return simItemSet\n",
    "\n",
    "\n",
    "\n",
    "def getItemToLatentFactorVec(item_user_data):\n",
    "    alpha = 10\n",
    "    training_data, test_data, altered_users = make_train(item_user_data)\n",
    "    \n",
    "    model = implicit.als.AlternatingLeastSquares(factors=15, regularization=0.1, iterations=10)\n",
    "    training_data = training_data.astype(np.float)        \n",
    "    model.fit(training_data*alpha)\n",
    "    \n",
    "    item_recommend = get_random_element_filtered(item_user_data)\n",
    "    similar_items, similar_scores = get_similar_items(model, item_recommend, 10)\n",
    "    \n",
    "    item_vecs = model.item_factors\n",
    "    user_vecs = model.user_factors\n",
    "    \n",
    "    cf_auc, baseline_auc = calc_mean_auc(training_data, altered_users,[csr_matrix(item_vecs), csr_matrix(user_vecs.T)], test_data)\n",
    "    \n",
    "    print \"Collaborative filtering AUC %.3f, Popularity baseline AUC %.3f\" % (cf_auc, baseline_auc)\n",
    "    return item_vecs#cf_auc, baseline_auc\n",
    "\n",
    "\n",
    "\n",
    "def getTopNchain(data, setsize): \n",
    "    data['chain_name'] = data.apply(lambda row: parseChainName(row.chain_info), axis=1)  # axis=0 - vertically, axis=1 - horizontally\n",
    "    chain_count = data['chain_name'].value_counts()\n",
    "    t = dict(chain_count)\n",
    "    \n",
    "    # sort dictionary using value\n",
    "    sorted_t = sorted(t.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    type(sorted_t)\n",
    "    \n",
    "    # top most frequent chain, for example top 30 appeared chain \n",
    "    s = set()\n",
    "    topN = setsize\n",
    "    for entry in sorted_t:\n",
    "        if(topN > 0):\n",
    "            s.add(entry[0])\n",
    "            topN = topN - 1\n",
    "    \n",
    "    print(s)        \n",
    "    return s   \n",
    "\n",
    "\n",
    "\n",
    "def initData(): \n",
    "    data = readDataset()\n",
    "    data['rating'] = 1\n",
    "    data['cookie_int'] = pd.factorize(data.cookie)[0]\n",
    "    data['ppid_int'] = pd.factorize(data.ppid)[0]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def initDataWithHeader(dataset, header): \n",
    "    data = readData(dataset, header)\n",
    "    data['rating'] = 1\n",
    "    data['cookie_int'] = pd.factorize(data.cookie)[0]\n",
    "    data['ppid_int'] = pd.factorize(data.ppid)[0]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def extendDataFrame(data):\n",
    "    data['chain_id'] = data.apply(lambda row: parseChainId(row.chain_info, row.ppid), axis=1)  \n",
    "    data_w_chain = data[data.chain_id != '-1']\n",
    "    data_w_chain['chain_int'] = pd.factorize(data_w_chain.chain_id)[0] \n",
    "    \n",
    "    return data_w_chain\n",
    "\n",
    "\n",
    "\n",
    "def extendDataFrameWithChainAndQuery(data, s, score):\n",
    "    data['chain_id'] = data.apply(lambda row: parseChainIdwithQuerySimilarity(row.query_string, row.chain_info, row.ppid, s, score), axis=1)\n",
    "    data_w_chain = data[data.chain_id != '-1']\n",
    "    data_w_chain['chain_int'] = pd.factorize(data_w_chain.chain_id)[0] \n",
    "    \n",
    "    return data_w_chain\n",
    "\n",
    "\n",
    "\n",
    "def plotPerformance(cf_auc_list, baseline_auc_list, title):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('Auc')\n",
    "    plt.title(title)\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    ran = range(1, 11, 1) \n",
    "    cf = cf_auc_list \n",
    "    po = baseline_auc_list \n",
    "    plt.plot(ran, cf, 'r^', ran, po, 'b^')\n",
    "    ax.plot(ran, cf, label = 'CF-based')\n",
    "    ax.plot(ran, po, label='Popularity-based')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.axis([0, 12, 0.8, 1])\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "\n",
    "def assignLabelsOnItems(data, s, score): \n",
    "    data['place_label'] = data.apply(lambda row: labelOnItemUsingChainIdwithQuerySimilarity(row.query_string, row.chain_info, row.ppid, s, score), axis=1)\n",
    "    print ( data['place_label'].value_counts())\n",
    "    data['place_label'] = pd.factorize(data['place_label'])[0] \n",
    "    return data['place_label'] \n",
    "\n",
    "\n",
    "\n",
    "def plotOneCluster(X_embedded, x_val):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i in range(5698): #number of data points in plot (number of vectors)\n",
    "        if (X_embedded[i,0] < x_val): #-55): \n",
    "            plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"r\", cmap=plt.cm.get_cmap(\"Dark2\", 7)) \n",
    "        else: \n",
    "            plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"g\", cmap=plt.cm.get_cmap(\"Dark2\", 7)) \n",
    "    \n",
    "    plt.show() \n",
    "\n",
    "\n",
    "\n",
    "def getItemsInCluster(X_embedded, x_val):\n",
    "    item_index_in_cluster = []\n",
    "    \n",
    "    for i in range(5698): #number of data points in plot (number of vectors)\n",
    "        if (X_embedded[i,0] < -55): \n",
    "            item_index_in_cluster.append(i)\n",
    "    return item_index_in_cluster\n",
    "\n",
    "\n",
    "\n",
    "def getItemsPlaceName(item_index_in_dataset, item_index_in_cluster, item_index_from_org_dataset):\n",
    "    item_index_in_cluster #99\n",
    "    item_index_in_dataset = []\n",
    "    \n",
    "    for k in range(len(item_index_in_cluster)): \n",
    "        idx = item_index_in_cluster[k]\n",
    "        item_index_in_dataset.append(item_index_from_org_dataset[idx])\n",
    "    \n",
    "    all_places = []\n",
    "    for x in range(len(item_index_in_dataset)): \n",
    "        ppid_idx = item_index_in_dataset[x]\n",
    "        records_w_placeName = data[data['ppid_int'] == ppid_idx].place_name\n",
    "        #print(records_w_placeName.values[0])\n",
    "        all_places.append(records_w_placeName.values[0])\n",
    "    \n",
    "    from collections import Counter\n",
    "    ret = Counter(all_places) \n",
    "    return ret \n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# experiment with matrix \"chain_id (with ppid if no chain) x user\"\n",
    "#-----------------------------------------------------------------\n",
    "data_w_chain = extendDataFrame(initData())\n",
    "chain_user_matrix = constructUserPlaceMatrix(data_w_chain.rating, data_w_chain.chain_int, data_w_chain.cookie_int)\n",
    "fitAndPredict(chain_user_matrix)\n",
    "\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Find top N chain set and best similarity score \n",
    "#------------------------------------------------------\n",
    "data = initDataWithHeader(NY_DATA, NY_HEADER)\n",
    "\n",
    "cf_auc_list = []\n",
    "baseline_auc_list = []\n",
    "topNchain_range = range(5, 50, 5)\n",
    "similarity_score_range = np.arange(0.8, 1.05, 0.05) \n",
    "\n",
    "for topN in topNchain_range: \n",
    "    print \"current set size : \", topN \n",
    "    topNchains = getTopNchain(data, topN)\n",
    "    for score in similarity_score_range: \n",
    "        print \" current similarity score : \", score  \n",
    "        data_w_chain_feature = extendDataFrameWithChainAndQuery(data, topNchains, score)\n",
    "        chain_user_matrix = constructUserPlaceMatrix(data_w_chain_feature.rating, data_w_chain_feature.chain_int, data_w_chain_feature.cookie_int)\n",
    "        cf_auc, baseline_auc = fitAndPredict(chain_user_matrix)\n",
    "        cf_auc_list.append(cf_auc)\n",
    "        baseline_auc_list.append(baseline_auc)\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# t-SME experiment with topNChains = 5 , similarity_score = 0.8\n",
    "# get items' latent feature vectors        \n",
    "#-----------------------------------------------------------------\n",
    "data = initDataWithHeader(CHICAGO_DATA, CHICAGO_HEADER)\n",
    "\n",
    "topN = 5\n",
    "score = 0.8\n",
    "topNchains = getTopNchain(data, topN)\n",
    "data_w_chain_feature = extendDataFrameWithChainAndQuery(data, topNchains, score)\n",
    "labeledItems = assignLabelsOnItems(data_w_chain_feature, topNchains, score)\n",
    "chain_user_matrix = constructUserPlaceMatrix(data_w_chain_feature.rating, data_w_chain_feature.chain_int, data_w_chain_feature.cookie_int)\n",
    "item_latentFactor_vec = getItemToLatentFactorVec(chain_user_matrix)\n",
    "t = getAllItemsWithCounts(data_w_chain_feature.rating, data_w_chain_feature.chain_int, data_w_chain_feature.cookie_int)\n",
    "\n",
    "threshold = 5\n",
    "item_latent_filtered = []\n",
    "item_label_filtered = []\n",
    "for i in range(len(t)):\n",
    "    if(t[i] > threshold): \n",
    "        item_latent_filtered.append(item_latentFactor_vec[i])\n",
    "        item_label_filtered.append(labeledItems[i])\n",
    "\n",
    "\n",
    "item_latent_filtered_vec = np.asarray(item_latent_filtered)\n",
    "item_label_filtered_vec = np.asarray(item_label_filtered)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# visualize t-SNE result with perplexity value with colorbar \n",
    "#--------------------------------------------------------------\n",
    "plt.figure(figsize=(13,11))\n",
    "labels = item_label_filtered_vec\n",
    "\n",
    "perplex = 5\n",
    "X_embedded = TSNE(n_components=2, init='random',random_state=0, n_iter=1000, perplexity=perplex).fit_transform(item_latent_filtered_vec)\n",
    "X_embedded.shape #(17232, 2)\n",
    "X_embedded\n",
    "labels = item_label_filtered_vec  \n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels, cmap=plt.cm.get_cmap(\"Dark2\", 7))  \n",
    "plt.colorbar(ticks=range(7)) \n",
    "plt.show()\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# plot one t-SNE result with perplexity value with legend\n",
    "# each vector of item laten feature vectors is labeled with one of top 5 chain name\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "labels = item_label_filtered_vec\n",
    "\n",
    "perplex = 100\n",
    "X_embedded = TSNE(n_components=2, init='random',random_state=0, n_iter=1000, perplexity=perplex).fit_transform(item_latent_filtered_vec)\n",
    "X_embedded.shape #(17232, 2)\n",
    "X_embedded\n",
    "\n",
    "ip1_tsne_display = X_embedded \n",
    "labels_display = labels \n",
    "\n",
    "plt.figure(figsize=(13,11))\n",
    "\n",
    "classNames = ['None', 'McDonald', 'Dunkin Donut', 'Baskin Robbins', 'Panera Bread', 'Starbucks', 'Jewel']\n",
    "print_classes = 7\n",
    "cmap = plt.get_cmap('nipy_spectral')\n",
    "test_colors = colors;\n",
    "colors = (colors + np.ones(colors.shape))/2.0\n",
    "\n",
    "f = plt.figure(figsize=(13,13))\n",
    "\n",
    "for label, color, className in zip(xrange(0,7), test_colors, classNames):\n",
    "    plt.plot(ip1_tsne_display[labels_display == label, 0], ip1_tsne_display[labels_display == label, 1],\n",
    "            'o', markersize=7.5, label=className, color=color)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# CF using matrix (ppid x cookie) - without chain info \n",
    "#-------------------------------------------------------\n",
    "data = initDataWithHeader(CHICAGO_DATA, CHICAGO_HEADER)\n",
    "ppid_cookie_matrix = constructUserPlaceMatrix(data.rating, data.ppid_int, data.cookie_int)\n",
    "\n",
    "item_latentFactor_vec = getItemToLatentFactorVec(ppid_cookie_matrix)\n",
    "filtered = getAllItemsWithCounts(data.rating, data.ppid_int, data.cookie_int)\n",
    "\n",
    "threshold = 5\n",
    "item_latent_filtered = []\n",
    "for i in range(len(filtered)):\n",
    "    print i\n",
    "    if(filtered[i] > threshold): \n",
    "        item_latent_filtered.append(item_latentFactor_vec[i])\n",
    "\n",
    "\n",
    "item_latent_filtered_vec = np.asarray(item_latent_filtered)\n",
    "item_latent_filtered_vec.shape \n",
    "\n",
    "# visualization of item_latent_filtered_vec\n",
    "(fig, subplots) = plt.subplots(4, 1, figsize=(15, 40))\n",
    "perplexities = [5, 30, 50, 100]\n",
    "colors_list = list(colors.cnames) \n",
    "color = colors_list[0:15] \n",
    "\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    ax = subplots[i]\n",
    "    X_embedded = TSNE(n_components=2, init='random',random_state=0, perplexity=perplexity).fit_transform(item_latent_filtered_vec)\n",
    "    X_embedded.shape #(17232, 2)\n",
    "    X_embedded\n",
    "    \n",
    "    ax.set_title(\"Perplexity=%d\" % perplexity)\n",
    "    ax.scatter(X_embedded[:, 0], X_embedded[:, 1], c=color) \n",
    "    ax.xaxis.set_major_formatter(NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(NullFormatter())\n",
    "    ax.axis('tight')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# CF using matrix 'item(=chain_info) x cookie'\n",
    "# Show the similar items of 'starbucks'\n",
    "#-----------------------------------------------------------------------------------------\n",
    "data = initDataWithHeader(CHICAGO_DATA, CHICAGO_HEADER)\n",
    "data\n",
    "\n",
    "topN = 5\n",
    "score = 0.0\n",
    "topNchains = getTopNchain(data, topN)\n",
    "data_w_chain_feature = extendDataFrameWithChainAndQuery(data, topNchains, score)\n",
    "#data_w_chain_feature[data_w_chain_feature['place_name'] == 'Starbucks']\n",
    "chain_user_matrix = constructUserPlaceMatrix(data_w_chain_feature.rating, data_w_chain_feature.chain_int, data_w_chain_feature.cookie_int)\n",
    "starbucks_chain_id = 225 #you need to know what is chain id in advance\n",
    "similarChains = getSimilarChainItems(chain_user_matrix, starbucks_chain_id) \n",
    "similarChains\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# CF using matrix (ppid x cookie) - without chain info\n",
    "#-------------------------------------------------------\n",
    "data = initDataWithHeader(CHICAGO_DATA, CHICAGO_HEADER)\n",
    "\n",
    "ppid_cookie_matrix = constructUserPlaceMatrix(data.rating, data.ppid_int, data.cookie_int)\n",
    "item_latentFactor_vec = getItemToLatentFactorVec(ppid_cookie_matrix)\n",
    "item_latentFactor_vec.shape # (17232, 15)\n",
    "\n",
    "filtered = getAllItemsWithCounts(data.rating, data.ppid_int, data.cookie_int)\n",
    "#filtered #dictionary , len = 17232\n",
    "\n",
    "threshold = 5 # user click number threshold\n",
    "item_latent_filtered = []\n",
    "item_index_from_org_dataset = []\n",
    "\n",
    "for i in range(len(filtered)):\n",
    "    if(filtered[i] > threshold):\n",
    "        item_index_from_org_dataset.append(i) #keep original index in dataset\n",
    "        item_latent_filtered.append(item_latentFactor_vec[i])\n",
    "\n",
    "\n",
    "item_index_from_org_dataset # item_index_from_org_dataset [i] will tell the index from original dataset\n",
    "len(item_index_from_org_dataset) #5698\n",
    "\n",
    "item_latent_filtered_vec = np.asarray(item_latent_filtered)\n",
    "item_latent_filtered_vec.shape #(5698, 15)\n",
    "\n",
    "\n",
    "#-----------\n",
    "# t-SNE\n",
    "#-----------\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "perplex = 100\n",
    "X_embedded = TSNE(n_components=2, init='random',random_state=0, n_iter=1000, perplexity=perplex).fit_transform(item_latent_filtered_vec)\n",
    "X_embedded.shape #(5698, 2) #is it same size as 'item_latent_filtered_vec' \n",
    "\n",
    "#----------------------------------------------\n",
    "# one cluster with different color (left most) \n",
    "#----------------------------------------------\n",
    "item_index_in_cluster = []\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(5698): #number of data points in plot (number of vectors)\n",
    "    #print X_embedded[i,0]\n",
    "    if (X_embedded[i,0] < -55): \n",
    "        item_index_in_cluster.append(i)\n",
    "        plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"r\", cmap=plt.cm.get_cmap(\"Dark2\", 7)) \n",
    "    else: \n",
    "        plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"g\", cmap=plt.cm.get_cmap(\"Dark2\", 7)) \n",
    "\n",
    "\n",
    "plt.show() \n",
    "\n",
    "x_val = -55\n",
    "plotOneCluster(X_embedded, x_val)\n",
    "items = getItemsInCluster(X_embedded, x_val)\n",
    "items    \n",
    "\n",
    "#----------------------------------------------\n",
    "# one cluster with different color (right most) \n",
    "#----------------------------------------------\n",
    "item_index_in_cluster_most_right = []\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(5698): #number of data points in plot (number of vectors)\n",
    "    #print X_embedded[i,0]\n",
    "    if (X_embedded[i,0] > 44): \n",
    "        item_index_in_cluster_most_right.append(i)\n",
    "        plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"orange\", cmap=plt.cm.get_cmap(\"Dark2\", 7)) \n",
    "    elif (X_embedded[i,0] < -45): \n",
    "            item_index_in_cluster.append(i)\n",
    "            plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"r\", cmap=plt.cm.get_cmap(\"Dark2\", 7))  \n",
    "    else: \n",
    "        plt.scatter(X_embedded[i, 0], X_embedded[i, 1], c=\"g\", cmap=plt.cm.get_cmap(\"Dark2\", 7)) \n",
    "\n",
    "\n",
    "plt.show() \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# extract indices within cluster and get original indices from dataset\n",
    "#------------------------------------------------------------------------                \n",
    "# get original item index \n",
    "len(item_index_from_org_dataset)    \n",
    "item_index_in_cluster #99\n",
    "item_index_in_dataset = []\n",
    "\n",
    "for k in range(len(item_index_in_cluster)): \n",
    "    idx = item_index_in_cluster[k]\n",
    "    item_index_in_dataset.append(item_index_from_org_dataset[idx])\n",
    "\n",
    "\n",
    "place_name_set = set() \n",
    "all_places = []\n",
    "for x in range(len(item_index_in_dataset)): \n",
    "    ppid_idx = item_index_in_dataset[x]\n",
    "    records_w_placeName = data[data['ppid_int'] == ppid_idx].place_name\n",
    "    all_places.append(records_w_placeName.values[0])\n",
    "    ppid = data[data['ppid_int'] == ppid_idx].ppid\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "Counter(all_places)   \n",
    "\n",
    "result = getItemsPlaceName(item_index_in_dataset, item_index_in_cluster, item_index_from_org_dataset)\n",
    "result\n",
    "\n",
    "#------------------------------------------\n",
    "# only 'Panera Bread' and plot on map\n",
    "pane_place_name_set = set() \n",
    "pane_all_places = []\n",
    "pane_lat = []\n",
    "pane_lon = []\n",
    "for x in range(len(item_index_in_dataset)): \n",
    "    ppid_idx = item_index_in_dataset[x]\n",
    "    records_w_placeName = data[data['ppid_int'] == ppid_idx].place_name\n",
    "    if(records_w_placeName.values[0] == 'Panera Bread'): \n",
    "        pane_lat.append(data[data['ppid_int'] == ppid_idx].lat)\n",
    "        pane_lon.append(data[data['ppid_int'] == ppid_idx].lon)\n",
    "\n",
    "\n",
    "for nLat in range(96) : \n",
    "    print pane_lat[nLat].values[0]\n",
    "    print pane_lon[0].values[0]\n",
    "\n",
    "\n",
    "# plot in map \n",
    "from gmplot import gmplot\n",
    "center_lat = np.mean(pane_lat[0])\n",
    "center_lon = np.mean(pane_lon[0])\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(center_lat, center_lon, 18)\n",
    "\n",
    "for i in range(96):\n",
    "    gmap.scatter(pane_lat[i], pane_lon[i], '#FF0000', size=18, marker=False)\n",
    "\n",
    "\n",
    "# show items on map and save to map file\n",
    "gmap.draw(\"C:/Users/shong/Documents/data/panera_place1.html\")\n",
    "\n",
    "#----------------------------------------------------\n",
    "# example with far right cluster from t-SNE result\n",
    "#----------------------------------------------------               \n",
    "# get original item index \n",
    "rmost_item_index_in_dataset = []\n",
    "\n",
    "for k in range(len(item_index_in_cluster_most_right)): \n",
    "    idx = item_index_in_cluster_most_right[k]\n",
    "    rmost_item_index_in_dataset.append(item_index_from_org_dataset[idx])\n",
    "\n",
    "\n",
    "rmost_place_name_set = set() \n",
    "rmost_all_places = []\n",
    "for x in range(len(rmost_item_index_in_dataset)): \n",
    "    ppid_idx = rmost_item_index_in_dataset[x]\n",
    "    records_w_placeName = data[data['ppid_int'] == ppid_idx].place_name\n",
    "    rmost_all_places.append(records_w_placeName.values[0])\n",
    "    ppid = data[data['ppid_int'] == ppid_idx].ppid\n",
    "    place_name_str =  records_w_placeName.values[0]\n",
    "    rmost_place_name_set.add(place_name_str)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "Counter(rmost_all_places)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
